{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/milindsoorya/colab-notebooks/blob/main/VariationalAutoEncoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Pct1rHqKdJ"
      },
      "source": [
        "Using Fashion MNIST dataset. Train VAR to generate pictures of clothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB6ulmuuo1Oh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mySedFJCr34e"
      },
      "outputs": [],
      "source": [
        "SEED = 12345\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']= '1'\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FW78LD-st3Z"
      },
      "source": [
        "## Exploring FashionMNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8isDmZYsqwe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qquQTTnstUKf"
      },
      "outputs": [],
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhTVRIxztfE1"
      },
      "outputs": [],
      "source": [
        "# Visualising the data\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(9, 9))\n",
        "\n",
        "# Choose 9 samples out of 60k available train set\n",
        "randSamples = np.random.choice(60000, 9)\n",
        "\n",
        "for i in range(9):\n",
        "  plt.subplot(3, 3, i+1)\n",
        "  plt.imshow(x_train[randSamples[i]], cmap='Greys_r')\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0M-kMrjumDM"
      },
      "outputs": [],
      "source": [
        "# As we are not bothered with classification, we can combine the train and test data.\n",
        "dataset = np.concatenate([x_train, x_test], axis=0)\n",
        "# Add extra dimension as the convolution layer expects 3 channels, 28x28 --> 28x28x1\n",
        "# Also normalising the value to [0, 1]\n",
        "dataset = np.expand_dims(dataset, -1).astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ons2uNx2vgng"
      },
      "outputs": [],
      "source": [
        "# Custom sampling layer\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a sampling layer\n",
        "class SamplingLayer(layers.Layer):\n",
        "  '''Reparameterization Trick z - mu + sigma * epsilon'''\n",
        "\n",
        "  def call(self, inputs):\n",
        "    zMean, zLogVar = inputs\n",
        "    batch = tf.shape(zMean)[0]\n",
        "    dim = tf.shape(zMean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return zMean + tf.exp(0.5 * zLogVar) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sgkoT1YKEoe"
      },
      "source": [
        "## Encoder / Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEnCzKT7Jy_9"
      },
      "outputs": [],
      "source": [
        "def buildEncoder(latentDim, encoderInputs):\n",
        "\n",
        "  # Given a block of images the convolutional block extract the features\n",
        "  l1 = keras.models.Sequential([\n",
        "    layers.Conv2D(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\")\n",
        "  ])\n",
        "\n",
        "  # Pass the inputs through the convolutional block\n",
        "  x = l1(encoderInputs)\n",
        "\n",
        "  # A dedicated layer to learn mean in parallel\n",
        "  zMean = layers.Dense(latentDim, name=\"z_mean\")(x)\n",
        "\n",
        "  # S dedicated layer to learn variance in parallel\n",
        "  zLogVar = layers.Dense(latentDim, name='z_log_var')(x)\n",
        "\n",
        "  # Now the reparameterization trick to find z as defined by mean and variance\n",
        "  z = SamplingLayer()([zMean, zLogVar])\n",
        "\n",
        "  # The actual model which takes the imahes as input and returns mean, variance and distribution\n",
        "  # The zMean and zLogVar are used in the KL divergence loss\n",
        "  return keras.Model(encoderInputs, [zMean, zLogVar, z], name=\"encoder\")\n",
        "\n",
        "# Trigger the function to actually build the model\n",
        "encoderInputs = keras.Input(shape=(28, 28, 1))\n",
        "encoder = buildEncoder(2, encoderInputs)\n",
        "encoder.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETsu8MuRWhVB"
      },
      "source": [
        "### Implementing the decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQwAG3kUV5zJ"
      },
      "outputs": [],
      "source": [
        "def buildDecoder(latentInputs):\n",
        "\n",
        "    l1 = keras.models.Sequential([\n",
        "      layers.Dense(7*7*64, activation=\"relu\", input_shape=(latentInputs.shape[1],)),\n",
        "      layers.Reshape((7, 7, 64)),\n",
        "      layers.Conv2DTranspose(128, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "      layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "      layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
        "    ])\n",
        "    \n",
        "    return keras.Model(latentInputs, l1(latentInputs), name=\"decoder\")\n",
        "\n",
        "# Build the model\n",
        "latentInputs = keras.Input(shape=(2,))\n",
        "decoder = buildDecoder(latentInputs)\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEiPQ7XJr1rS"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(encoder, \"encoder.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppLH6P7pr-Ia"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(decoder, \"decoder.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vibpSReGY3eP"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "overall goal is to make the decoder generate image as close to the image fed into the auto encoder. \n",
        "\n",
        "**Reconstruction loss** \n",
        "- It penalises the images that are not similar to the original.\n",
        "\n",
        "- It works by comparing the binary cross entropy.\n",
        "\n",
        "**kl Divergence loss** \n",
        "- It is looking at the distance between the probability distribution from the generated and the original image. The aim is the make the distance as close as posssible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIjugax4Yr3K"
      },
      "outputs": [],
      "source": [
        "def reconstructionLoss(data, reconstructed):\n",
        "  return tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          keras.losses.binary_crossentropy(data, reconstructed),\n",
        "          axis=(1, 2)\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAWKl4MVaU9m"
      },
      "outputs": [],
      "source": [
        "def klDivergenceLoss(zMean, zLogVar):\n",
        "  return tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "         -0.5 * (1 + zLogVar -tf.square(zMean) - tf.exp(zLogVar)),\n",
        "          axis=1\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLWx6w7WavSG"
      },
      "source": [
        "Now combine these into the total loss function, which just weights them and sums them up. Think pf that weight as another hyperparameter you can tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7E9GJ_caryf"
      },
      "outputs": [],
      "source": [
        "def calcTotalLoss(data, reconstructed, zMean, zLogVar):\n",
        "  loss1 = reconstructionLoss(data, reconstructed)\n",
        "  loss2 = klDivergenceLoss(zMean, zLogVar)\n",
        "  klweight = 3.0\n",
        "  return loss1, loss2, loss1 + klweight * loss2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KprxYh63bjLV"
      },
      "source": [
        "## Overriding train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0FWb9l7bh-O"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "  def __init__(self, encoder, decoder, **kwargs):\n",
        "      super(VAE, self).__init__(**kwargs)\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      # Register total loss as an observable metric in the model training history\n",
        "      self.totalLossTracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "      self.ceLossTracker = keras.metrics.Mean(name=\"ce_loss\")\n",
        "      self.klLossTracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "  # These are all observable metrics\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [\n",
        "            self.totalLossTracker,\n",
        "            self.ceLossTracker,\n",
        "            self.klLossTracker\n",
        "    ]\n",
        "\n",
        "  # Now calculate the loss, gradients and update the weights\n",
        "  def train_step(self, data):\n",
        "    # Gradient tape is a recording of all gradients for the trainable \n",
        "    # weights that need to be updated\n",
        "    with tf.GradientTape() as tape:\n",
        "      # forwards path\n",
        "      zMean, zLogVar, z = self.encoder(data)\n",
        "      reconstruction = self.decoder(z)\n",
        "      ceLoss, klLoss, totalLoss = calcTotalLoss(data, reconstruction, zMean, zLogVar)\n",
        "    # Backward path\n",
        "    grads = tape.gradient(totalLoss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "    # keep track of loss\n",
        "    self.totalLossTracker.update_state(totalLoss)\n",
        "    self.ceLossTracker.update_state(ceLoss)\n",
        "    self.klLossTracker.update_state(klLoss)\n",
        "\n",
        "    # Return the loss for history object\n",
        "    return {\n",
        "        \"total_loss\": self.totalLossTracker.result(),\n",
        "        \"ce_loss\": self.ceLossTracker.result(),\n",
        "        \"kl_loss\": self.klLossTracker.result()\n",
        "    } "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uj6YwK0lMwa"
      },
      "source": [
        "### Train the VAE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTixAtHljdfv",
        "outputId": "b4aa3bd6-df00-4715-8652-d859a8e9592d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "547/547 [==============================] - 23s 17ms/step - total_loss: 312.9810 - ce_loss: 301.4705 - kl_loss: 3.8368\n",
            "Epoch 2/32\n",
            "547/547 [==============================] - 9s 17ms/step - total_loss: 279.3307 - ce_loss: 265.6431 - kl_loss: 4.5625\n",
            "Epoch 3/32\n",
            "547/547 [==============================] - 11s 20ms/step - total_loss: 276.3748 - ce_loss: 262.4290 - kl_loss: 4.6486\n",
            "Epoch 4/32\n",
            "547/547 [==============================] - 9s 17ms/step - total_loss: 274.7228 - ce_loss: 260.5677 - kl_loss: 4.7183\n",
            "Epoch 5/32\n",
            "547/547 [==============================] - 11s 19ms/step - total_loss: 273.6249 - ce_loss: 259.2971 - kl_loss: 4.7758\n",
            "Epoch 6/32\n",
            "547/547 [==============================] - 10s 19ms/step - total_loss: 272.6124 - ce_loss: 258.1545 - kl_loss: 4.8193\n",
            "Epoch 7/32\n",
            "547/547 [==============================] - 10s 19ms/step - total_loss: 271.8624 - ce_loss: 257.3163 - kl_loss: 4.8487\n",
            "Epoch 8/32\n",
            "547/547 [==============================] - 10s 18ms/step - total_loss: 271.4424 - ce_loss: 256.7414 - kl_loss: 4.9003\n",
            "Epoch 9/32\n",
            "547/547 [==============================] - 11s 19ms/step - total_loss: 270.9596 - ce_loss: 256.1539 - kl_loss: 4.9352\n",
            "Epoch 10/32\n",
            "357/547 [==================>...........] - ETA: 3s - total_loss: 270.3747 - ce_loss: 255.5052 - kl_loss: 4.9565"
          ]
        }
      ],
      "source": [
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
        "history = vae.fit(dataset, batch_size=128, epochs=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#First you access the learnt weights of the encoder and decoder from the VAE model and save them\n",
        "vae.get_layer('encoder').save_weights('encoder_weights.h5')\n",
        "vae.get_layer('decoder').save_weights('decoder_weights.h5')\n",
        "\n",
        "\n",
        "#Since both encoder and decoder are treated as models, you also need to save their architecture defined via instantiated VAE model\n",
        "vae.get_layer('encoder').save('encoder_arch') #If you are using Google Colab they are saved in a folders: decoder_arch\n",
        "vae.get_layer('decoder').save('decoder_arch') #and encoder_arch."
      ],
      "metadata": {
        "id": "dDImjOolqv-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "#TIP#\n",
        "#####\n",
        "\n",
        "#If you are using Google Colab you can download the folders (such as decoder_arch and encoder_arch) by first zipping them and then \n",
        "#using the Google Colab functionality to download them (or do it manually ;))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/decoder_arch.zip /content/decoder_arch\n",
        "!zip -r /content/encoder_arch.zip /content/encoder_arch\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/encoder_arch.zip')\n",
        "files.download('/content/decoder_arch.zip')"
      ],
      "metadata": {
        "id": "J5UueveAq_Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets load the model in new VAE and also the corresponding weights. Upload the folders and .hd5 files into Colab\n",
        "\n",
        "encoder_new = keras.models.load_model('encoder_arch') #Loading the encoder model\n",
        "decoder_new = keras.models.load_model('decoder_arch') #Loading the decoder model\n",
        "\n",
        "vae_new = VAE(encoder_new, decoder_new) #You need to have VAE class defined for this to works\n",
        "vae_new.get_layer('encoder').load_weights('encoder_weights.h5') #On a given encoder model defined by vae_new we want to load the weights\n",
        "vae_new.get_layer('decoder').load_weights('decoder_weights.h5') #for encoder and decoder\n",
        "vae_new.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001)) #Now we need to compile the model and we are ready to go!"
      ],
      "metadata": {
        "id": "D5mTAZZNrFHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUkqGIApwh8Z"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 9))\n",
        "plt.plot(history.history.get('total_loss'), label=\"total loss\")\n",
        "plt.plot(history.history.get('ce_loss'), label=\"reconstruction loss\")\n",
        "plt.plot(history.history.get('kl_loss'), label=\"KL loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQu0RUNmxDVB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 9))\n",
        "plt.plot(history.history.get('kl_loss'), label=\"KL loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSQ7iOw-xcSN"
      },
      "source": [
        "## Explore the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm2hWS-gxnH7"
      },
      "outputs": [],
      "source": [
        "synth = vae.decoder.predict([[1, 2]])\n",
        "plt.imshow(synth[0].reshape((28, 28)), cmap=\"Greys_r\")\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UC5pEd8yAHv"
      },
      "outputs": [],
      "source": [
        "z = np.random.normal(loc=0, scale=4, size=(256, 2))\n",
        "synth = vae.decoder.predict(z)\n",
        "\n",
        "plt.figure(figsize=(28, 28))\n",
        "\n",
        "for i in range(256):\n",
        "  plt.subplot(16, 16, i+1)\n",
        "  plt.imshow(synth[i].reshape((28, 28)), cmap=\"Greys_r\")\n",
        "  plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQzIyUTT11L_"
      },
      "source": [
        "## Drawing a specific kind of clothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVMSTK480mcs"
      },
      "outputs": [],
      "source": [
        "idx = 1200\n",
        "batch = np.expand_dims(x_train[idx], axis=0)\n",
        "batchOfImages = np.expand_dims(batch, axis=-1).astype(\"float32\") / 255\n",
        "print(batchOfImages.shape)\n",
        "\n",
        "# Obtain z(mu, sigma) for the given image\n",
        "_, _, z = vae.encoder.predict(batchOfImages)\n",
        "\n",
        "# Now reconstruct a similar image\n",
        "synth = vae.decoder.predict([z])\n",
        "\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(28, 28))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.axis('off')\n",
        "plt.imshow(x_train[idx], cmap=\"Greys_r\")\n",
        "\n",
        "# Reconstructed\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.axis('off')\n",
        "plt.imshow(synth[0].reshape((28, 28)), cmap=\"Greys_r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UYfMCuWTiGb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE can be used for clusturing for large text or unlabelled images corpus\n",
        "labels = np.concatenate([y_train, y_test], axis=0)\n",
        "meu, _, _ = vae.encoder.predict(dataset)\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.scatter(meu[:, 0], meu[:, 1], c=labels)\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"meu[0]\")\n",
        "plt.ylabel(\"meu[1]\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VqMi2HtCjDsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods to save VAE Model"
      ],
      "metadata": {
        "id": "dZYFIOSsWUXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# https://www.reddit.com/r/learnmachinelearning/comments/t4dbmb/how_to_save_vae_model_made_by_keras/\n",
        "#First you access the learnt weights of the encoder and decoder from the VAE model and save them\n",
        "your_vae_model.get_layer('encoder').save_weights('encoder_weights.h5')\n",
        "your_vae_model.get_layer('decoder').save_weights('decoder_weights.h5')\n",
        "\n",
        "\n",
        "#Since both encoder and decoder are treated as models, you also need to save their architecture defined via instantiated VAE model\n",
        "your_vae_model.get_layer('encoder').save('encoder_arch') #If you are using Google Colab they are saved in a folders: decoder_arch\n",
        "your_vae_model.get_layer('decoder').save('decoder_arch') #and encoder_arch.\n",
        "\n",
        "\n",
        "######\n",
        "#TIP#\n",
        "#####\n",
        "\n",
        "#If you are using Google Colab you can download the folders (such as decoder_arch and encoder_arch) by first zipping them and then \n",
        "#using the Google Colab functionality to download them (or do it manually ;))\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!zip -r /content/decoder_arch.zip /content/decoder_arch\n",
        "!zip -r /content/encoder_arch.zip /content/encoder_arch\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/encoder_arch.zip')\n",
        "files.download('/content/decoder_arch.zip')\n",
        "\n",
        "#Lets load the model in new VAE and also the corresponding weights. Upload the folders and .hd5 files into Colab\n",
        "\n",
        "encoder_new = keras.models.load_model('encoder_arch') #Loading the encoder model\n",
        "decoder_new = keras.models.load_model('decoder_arch') #Loading the decoder model\n",
        "\n",
        "vae_new = VAE(encoder_new, decoder_new) #You need to have VAE class defined for this to works\n",
        "vae_new.get_layer('encoder').load_weights('encoder_weights.h5') #On a given encoder model defined by vae_new we want to load the weights\n",
        "vae_new.get_layer('decoder').load_weights('decoder_weights.h5') #for encoder and decoder\n",
        "vae_new.compile(optimizer=keras.optimizers.Adam()) #Now we need to compile the model and we are ready to go!\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "DS_1YVIOpdvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# loading dependency\n",
        "import joblib\n",
        "\n",
        "# saving our model # model - model , filename-model_jlib\n",
        "joblib.dump(history.history, 'model_jlib')\n",
        "\n",
        "# opening the file- model_jlib\n",
        "vae = joblib.load('model_jlib')\n",
        "\n",
        "\n",
        "# loading library\n",
        "import pickle\n",
        "\n",
        "# create an iterator object with write permission - model.pkl\n",
        "with open('model_pkl', 'wb') as files:\n",
        "    pickle.dump(vae, files)\n",
        "\n",
        "# load saved model\n",
        "with open('model_pkl' , 'rb') as f:\n",
        "    vae = pickle.load(f)\n",
        "'''"
      ],
      "metadata": {
        "id": "jonS5PYSquog"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VariationalAutoEncoders.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObDEIfVzpl3Ww/C6O9Ct35",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}